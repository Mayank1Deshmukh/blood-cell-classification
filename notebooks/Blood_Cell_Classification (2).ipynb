{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU49HUdckIID",
        "outputId": "a1f8c525-3227-4ee0-b1db-2bc9e7c1332a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'blood-cell-classification' already exists and is not an empty directory.\n",
            "/content/blood-cell-classification/blood-cell-classification\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/ankitsunil530/blood-cell-classification.git\n",
        "# %cd blood-cell-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiGH1t3MkQ8D",
        "outputId": "3119b7e3-ce71-43d5-fc1f-d12300e470d6"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o77FxEgYMI7U"
      },
      "source": [
        "# **Blood Cell Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1bd6044"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Load the blood cell dataset and preprocess the images for use with the vision transformer and Performer models. This will involve resizing, normalization, and splitting the data into training, validation, and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e819139e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opencv-python\n",
            "  Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
            "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
            "  Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
            "Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
            "   ---------------------------------------- 0.0/39.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/39.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.3/39.0 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.5/39.0 MB 1.3 MB/s eta 0:00:30\n",
            "    --------------------------------------- 0.8/39.0 MB 1.8 MB/s eta 0:00:22\n",
            "   - -------------------------------------- 1.3/39.0 MB 1.8 MB/s eta 0:00:21\n",
            "   - -------------------------------------- 1.6/39.0 MB 1.5 MB/s eta 0:00:25\n",
            "   -- ------------------------------------- 2.4/39.0 MB 2.0 MB/s eta 0:00:19\n",
            "   --- ------------------------------------ 3.1/39.0 MB 2.3 MB/s eta 0:00:16\n",
            "   ---- ----------------------------------- 3.9/39.0 MB 2.5 MB/s eta 0:00:15\n",
            "   ---- ----------------------------------- 4.7/39.0 MB 2.6 MB/s eta 0:00:14\n",
            "   ----- ---------------------------------- 5.2/39.0 MB 2.7 MB/s eta 0:00:13\n",
            "   ------ --------------------------------- 6.0/39.0 MB 2.8 MB/s eta 0:00:12\n",
            "   ------ --------------------------------- 6.8/39.0 MB 2.8 MB/s eta 0:00:12\n",
            "   ------- -------------------------------- 7.6/39.0 MB 2.9 MB/s eta 0:00:11\n",
            "   -------- ------------------------------- 8.4/39.0 MB 2.9 MB/s eta 0:00:11\n",
            "   --------- ------------------------------ 9.2/39.0 MB 3.0 MB/s eta 0:00:10\n",
            "   ---------- ----------------------------- 10.0/39.0 MB 3.0 MB/s eta 0:00:10\n",
            "   ----------- ---------------------------- 10.7/39.0 MB 3.1 MB/s eta 0:00:10\n",
            "   ----------- ---------------------------- 11.5/39.0 MB 3.1 MB/s eta 0:00:09\n",
            "   ------------ --------------------------- 12.1/39.0 MB 3.1 MB/s eta 0:00:09\n",
            "   ------------- -------------------------- 12.8/39.0 MB 3.1 MB/s eta 0:00:09\n",
            "   ------------- -------------------------- 13.6/39.0 MB 3.1 MB/s eta 0:00:09\n",
            "   -------------- ------------------------- 14.4/39.0 MB 3.2 MB/s eta 0:00:08\n",
            "   --------------- ------------------------ 15.5/39.0 MB 3.2 MB/s eta 0:00:08\n",
            "   ---------------- ----------------------- 16.0/39.0 MB 3.2 MB/s eta 0:00:08\n",
            "   ----------------- ---------------------- 16.8/39.0 MB 3.2 MB/s eta 0:00:07\n",
            "   ------------------ --------------------- 17.6/39.0 MB 3.2 MB/s eta 0:00:07\n",
            "   ------------------ --------------------- 18.4/39.0 MB 3.2 MB/s eta 0:00:07\n",
            "   ------------------- -------------------- 18.9/39.0 MB 3.2 MB/s eta 0:00:07\n",
            "   -------------------- ------------------- 19.7/39.0 MB 3.2 MB/s eta 0:00:06\n",
            "   -------------------- ------------------- 20.4/39.0 MB 3.3 MB/s eta 0:00:06\n",
            "   --------------------- ------------------ 21.2/39.0 MB 3.3 MB/s eta 0:00:06\n",
            "   ---------------------- ----------------- 22.3/39.0 MB 3.3 MB/s eta 0:00:06\n",
            "   ----------------------- ---------------- 22.5/39.0 MB 3.2 MB/s eta 0:00:06\n",
            "   ----------------------- ---------------- 23.3/39.0 MB 3.2 MB/s eta 0:00:05\n",
            "   ------------------------ --------------- 24.1/39.0 MB 3.3 MB/s eta 0:00:05\n",
            "   ------------------------- -------------- 24.6/39.0 MB 3.3 MB/s eta 0:00:05\n",
            "   -------------------------- ------------- 25.4/39.0 MB 3.3 MB/s eta 0:00:05\n",
            "   -------------------------- ------------- 26.2/39.0 MB 3.3 MB/s eta 0:00:04\n",
            "   --------------------------- ------------ 27.3/39.0 MB 3.3 MB/s eta 0:00:04\n",
            "   ---------------------------- ----------- 27.5/39.0 MB 3.3 MB/s eta 0:00:04\n",
            "   ----------------------------- ---------- 28.3/39.0 MB 3.3 MB/s eta 0:00:04\n",
            "   ----------------------------- ---------- 28.8/39.0 MB 3.3 MB/s eta 0:00:04\n",
            "   ------------------------------ --------- 29.6/39.0 MB 3.3 MB/s eta 0:00:03\n",
            "   ------------------------------- -------- 30.4/39.0 MB 3.3 MB/s eta 0:00:03\n",
            "   ------------------------------- -------- 31.2/39.0 MB 3.3 MB/s eta 0:00:03\n",
            "   -------------------------------- ------- 32.0/39.0 MB 3.3 MB/s eta 0:00:03\n",
            "   --------------------------------- ------ 32.8/39.0 MB 3.3 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 33.6/39.0 MB 3.3 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 34.3/39.0 MB 3.3 MB/s eta 0:00:02\n",
            "   ------------------------------------ --- 35.1/39.0 MB 3.3 MB/s eta 0:00:02\n",
            "   ------------------------------------ --- 35.9/39.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 36.4/39.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 37.0/39.0 MB 3.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 37.5/39.0 MB 3.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 37.7/39.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  38.3/39.0 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  38.5/39.0 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  38.8/39.0 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 39.0/39.0 MB 3.1 MB/s  0:00:12\n",
            "Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
            "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.3/12.9 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.8/12.9 MB 1.9 MB/s eta 0:00:07\n",
            "   ---- ----------------------------------- 1.3/12.9 MB 1.8 MB/s eta 0:00:07\n",
            "   ---- ----------------------------------- 1.6/12.9 MB 1.8 MB/s eta 0:00:07\n",
            "   ------ --------------------------------- 2.1/12.9 MB 2.0 MB/s eta 0:00:06\n",
            "   -------- ------------------------------- 2.6/12.9 MB 1.9 MB/s eta 0:00:06\n",
            "   -------- ------------------------------- 2.9/12.9 MB 1.9 MB/s eta 0:00:06\n",
            "   --------- ------------------------------ 3.1/12.9 MB 1.9 MB/s eta 0:00:06\n",
            "   ----------- ---------------------------- 3.7/12.9 MB 1.9 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 3.9/12.9 MB 1.8 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 4.5/12.9 MB 1.9 MB/s eta 0:00:05\n",
            "   ---------------- ----------------------- 5.2/12.9 MB 2.0 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 5.5/12.9 MB 2.0 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 6.3/12.9 MB 2.1 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 6.8/12.9 MB 2.1 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 7.3/12.9 MB 2.1 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 7.6/12.9 MB 2.1 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 7.9/12.9 MB 2.1 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 8.4/12.9 MB 2.0 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 8.7/12.9 MB 2.0 MB/s eta 0:00:03\n",
            "   ---------------------------- ----------- 9.2/12.9 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 9.7/12.9 MB 2.1 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 10.7/12.9 MB 2.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 11.5/12.9 MB 2.2 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 12.1/12.9 MB 2.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  12.6/12.9 MB 2.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.9/12.9 MB 2.3 MB/s  0:00:05\n",
            "Installing collected packages: numpy, opencv-python\n",
            "\n",
            "  Attempting uninstall: numpy\n",
            "\n",
            "    Found existing installation: numpy 1.26.4\n",
            "\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "    Uninstalling numpy-1.26.4:\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   ---------------------------------------- 0/2 [numpy]\n",
            "   -------------------- ------------------- 1/2 [opencv-python]\n",
            "   -------------------- ------------------- 1/2 [opencv-python]\n",
            "   -------------------- ------------------- 1/2 [opencv-python]\n",
            "   -------------------- ------------------- 1/2 [opencv-python]\n",
            "   -------------------- ------------------- 1/2 [opencv-python]\n",
            "   -------------------- ------------------- 1/2 [opencv-python]\n",
            "   -------------------- ------------------- 1/2 [opencv-python]\n",
            "   -------------------- ------------------- 1/2 [opencv-python]\n",
            "   -------------------- ------------------- 1/2 [opencv-python]\n",
            "   ---------------------------------------- 2/2 [opencv-python]\n",
            "\n",
            "Successfully installed numpy-2.2.6 opencv-python-4.12.0.88\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Sunil Kumar\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~umpy.libs'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Sunil Kumar\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~umpy'.\n",
            "  You can safely remove it manually.\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mlflow 2.5.0 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "caabfb6a"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"D:\\\\Deep Learning Project\\\\blood-cell-classification\\\\data\\\\raw\\\\archive (3)\"\n",
        "\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "\n",
        "def load_images_and_labels(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = sorted(os.listdir(dataset_path))\n",
        "    class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                img_path = os.path.join(class_dir, img_name)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "                    images.append(img)\n",
        "                    labels.append(class_to_idx[class_name])\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v91fwvHf3YN",
        "outputId": "50211248-6f50-496e-ad79-b335218cfc7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "archive (3)/\n",
            "    dataset-master/\n",
            "        dataset-master/\n",
            "            Annotations/\n",
            "            JPEGImages/\n",
            "    dataset2-master/\n",
            "        dataset2-master/\n",
            "            images/\n",
            "                TEST/\n",
            "                    EOSINOPHIL/\n",
            "                    LYMPHOCYTE/\n",
            "                    MONOCYTE/\n",
            "                    NEUTROPHIL/\n",
            "                TEST_SIMPLE/\n",
            "                    EOSINOPHIL/\n",
            "                    LYMPHOCYTE/\n",
            "                    MONOCYTE/\n",
            "                    NEUTROPHIL/\n",
            "                TRAIN/\n",
            "                    EOSINOPHIL/\n",
            "                    LYMPHOCYTE/\n",
            "                    MONOCYTE/\n",
            "                    NEUTROPHIL/\n"
          ]
        }
      ],
      "source": [
        "for root, dirs, files in os.walk(DATASET_PATH):\n",
        "    level = root.replace(DATASET_PATH, '').count(os.sep)\n",
        "    indent = ' ' * 4 * (level)\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0b77a45",
        "outputId": "3107e169-b8e7-4831-c9dd-07ec7a07276f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found classes: ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\n",
            "Processing directory: D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\\dataset2-master\\dataset2-master\\images\\TRAIN\\EOSINOPHIL\n",
            "Processing directory: D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\\dataset2-master\\dataset2-master\\images\\TRAIN\\LYMPHOCYTE\n",
            "Processing directory: D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\\dataset2-master\\dataset2-master\\images\\TRAIN\\MONOCYTE\n",
            "Processing directory: D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\\dataset2-master\\dataset2-master\\images\\TRAIN\\NEUTROPHIL\n",
            "Processing directory: D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\\dataset2-master\\dataset2-master\\images\\TEST\\EOSINOPHIL\n",
            "Processing directory: D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\\dataset2-master\\dataset2-master\\images\\TEST\\LYMPHOCYTE\n",
            "Processing directory: D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\\dataset2-master\\dataset2-master\\images\\TEST\\MONOCYTE\n",
            "Processing directory: D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\\dataset2-master\\dataset2-master\\images\\TEST\\NEUTROPHIL\n",
            "Processing directory: D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\\dataset2-master\\dataset2-master\\images\\TEST_SIMPLE\\EOSINOPHIL\n",
            "Processing directory: D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\\dataset2-master\\dataset2-master\\images\\TEST_SIMPLE\\LYMPHOCYTE\n",
            "Processing directory: D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\\dataset2-master\\dataset2-master\\images\\TEST_SIMPLE\\MONOCYTE\n",
            "Processing directory: D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\\dataset2-master\\dataset2-master\\images\\TEST_SIMPLE\\NEUTROPHIL\n",
            "Loaded 12515 images and 12515 labels.\n",
            "Training set size: 10012\n",
            "Validation set size: 1251\n",
            "Test set size: 1252\n",
            "Class names: ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\n"
          ]
        }
      ],
      "source": [
        "def load_images_and_labels(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    # Update base directories to reflect the actual structure\n",
        "    base_image_dirs = [\n",
        "        os.path.join('dataset2-master', 'dataset2-master', 'images', 'TRAIN'),\n",
        "        os.path.join('dataset2-master', 'dataset2-master', 'images', 'TEST'),\n",
        "        os.path.join('dataset2-master', 'dataset2-master', 'images', 'TEST_SIMPLE')\n",
        "    ]\n",
        "    class_names = set()\n",
        "\n",
        "    # First pass to collect all unique class names\n",
        "    for base_img_dir_name in base_image_dirs:\n",
        "        base_img_dir_path = os.path.join(dataset_path, base_img_dir_name)\n",
        "        if os.path.isdir(base_img_dir_path):\n",
        "            for class_name in os.listdir(base_img_dir_path):\n",
        "                class_dir_path = os.path.join(base_img_dir_path, class_name)\n",
        "                if os.path.isdir(class_dir_path):\n",
        "                    class_names.add(class_name)\n",
        "\n",
        "    class_names = sorted(list(class_names))\n",
        "    class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
        "\n",
        "    print(f\"Found classes: {class_names}\")\n",
        "\n",
        "    # Second pass to load images\n",
        "    for base_img_dir_name in base_image_dirs:\n",
        "        base_img_dir_path = os.path.join(dataset_path, base_img_dir_name)\n",
        "        if os.path.isdir(base_img_dir_path):\n",
        "            for class_name in os.listdir(base_img_dir_path):\n",
        "                class_dir_path = os.path.join(base_img_dir_path, class_name)\n",
        "                print(f\"Processing directory: {class_dir_path}\")\n",
        "                if os.path.isdir(class_dir_path) and class_name in class_to_idx:\n",
        "                    for img_name in os.listdir(class_dir_path):\n",
        "                        img_path = os.path.join(class_dir_path, img_name)\n",
        "                        if os.path.isfile(img_path) and img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                            img = cv2.imread(img_path)\n",
        "                            if img is not None:\n",
        "                                img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "                                images.append(img)\n",
        "                                labels.append(class_to_idx[class_name])\n",
        "                            else:\n",
        "                                print(f\"Could not load image: {img_path}\")\n",
        "\n",
        "    print(f\"Loaded {len(images)} images and {len(labels)} labels.\")\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "images, labels, class_names = load_images_and_labels(DATASET_PATH)\n",
        "\n",
        "# Split the data - combining images from TRAIN, TEST, TEST_SIMPLE before splitting\n",
        "# Using a fixed random state for reproducibility\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(f\"Class names: {class_names}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset classes: ['dataset-master', 'dataset2-master']\n",
            "Dataset size: 12881\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 198\u001b[0m\n\u001b[0;32m    196\u001b[0m output \u001b[38;5;241m=\u001b[39m netD(fake)\n\u001b[0;32m    197\u001b[0m errG \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[1;32m--> 198\u001b[0m \u001b[43merrG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m D_G_z2 \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    200\u001b[0m optimizerG\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\Sunil Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Sunil Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# train_gan.py\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets, utils\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "# --------------------\n",
        "# Config / Hyperparams\n",
        "# --------------------\n",
        "DATASET_PATH = r\"D:\\Deep Learning Project\\blood-cell-classification\\data\\raw\\archive (3)\"  # set correct path\n",
        "EXPERIMENT = \"Blood-Cell-GAN\"\n",
        "RUN_NAME = \"DCGAN-run1\"\n",
        "OUT_DIR = \"gan_outputs\"\n",
        "MODEL_DIR = \"models\"\n",
        "IMG_SIZE = 128                 # use 128 (faster than 224)\n",
        "BATCH_SIZE = 64\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 100\n",
        "LR = 2e-4\n",
        "BETA1 = 0.5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SAMPLE_EVERY = 5               # epochs\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# --------------------\n",
        "# Data preparation\n",
        "# --------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3),  # images normalized to [-1,1]\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=DATASET_PATH, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(\"Dataset classes:\", dataset.classes)\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "\n",
        "# --------------------\n",
        "# Models (DCGAN-style)\n",
        "# --------------------\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=100, ngf=64, nc=3):\n",
        "        super().__init__()\n",
        "        # for 128x128 output, we need more layers\n",
        "        self.net = nn.Sequential(\n",
        "            # input z -> (ngf*16) x 4 x 4\n",
        "            nn.ConvTranspose2d(z_dim, ngf*16, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf*16),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size ngf*16 x 4 x 4 -> ngf*8 x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf*16, ngf*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf*8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # -> ngf*4 x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf*4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # -> ngf*2 x 32 x 32\n",
        "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf*2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # -> ngf x 64 x 64\n",
        "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # -> nc x 128 x 128\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nc=3, ndf=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # input nc x 128 x 128\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf*4, ndf*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf*8, ndf*16, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*16),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # final\n",
        "            nn.Conv2d(ndf*16, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).view(-1, 1).squeeze(1)\n",
        "\n",
        "# instantiate\n",
        "netG = Generator(Z_DIM).to(DEVICE)\n",
        "netD = Discriminator().to(DEVICE)\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Loss & optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
        "\n",
        "fixed_noise = torch.randn(64, Z_DIM, 1, 1, device=DEVICE)  # for sample grid\n",
        "\n",
        "# --------------------\n",
        "# MLflow setup\n",
        "# --------------------\n",
        "mlflow.set_experiment(EXPERIMENT)\n",
        "mlflow.start_run(run_name=RUN_NAME)\n",
        "mlflow.log_params({\n",
        "    \"img_size\": IMG_SIZE, \"batch_size\": BATCH_SIZE, \"z_dim\": Z_DIM,\n",
        "    \"lr\": LR, \"beta1\": BETA1, \"epochs\": NUM_EPOCHS, \"device\": str(DEVICE)\n",
        "})\n",
        "\n",
        "# --------------------\n",
        "# Training loop\n",
        "# --------------------\n",
        "real_label_val = 1.0\n",
        "fake_label_val = 0.0\n",
        "\n",
        "iters = 0\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    epoch_start = time.time()\n",
        "    running_D_loss = 0.0\n",
        "    running_G_loss = 0.0\n",
        "    for i, (data, _) in enumerate(dataloader):\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data.to(DEVICE)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label_val, dtype=torch.float, device=DEVICE)\n",
        "\n",
        "        # Train D on real\n",
        "        output = netD(real_cpu)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        # Train D on fake\n",
        "        noise = torch.randn(b_size, Z_DIM, 1, 1, device=DEVICE)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label_val)\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        # Train G\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label_val)  # want generator to produce real labels\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        running_D_loss += errD.item()\n",
        "        running_G_loss += errG.item()\n",
        "\n",
        "        # log per-batch occasionally (avoid too many logs)\n",
        "        if iters % 100 == 0:\n",
        "            mlflow.log_metric(\"D_loss_batch\", errD.item(), step=iters)\n",
        "            mlflow.log_metric(\"G_loss_batch\", errG.item(), step=iters)\n",
        "\n",
        "        iters += 1\n",
        "\n",
        "    # epoch stats\n",
        "    avg_D = running_D_loss / len(dataloader)\n",
        "    avg_G = running_G_loss / len(dataloader)\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}]  D_loss: {avg_D:.4f}  G_loss: {avg_G:.4f}  time: {epoch_time:.1f}s\")\n",
        "\n",
        "    mlflow.log_metric(\"D_loss_epoch\", avg_D, step=epoch)\n",
        "    mlflow.log_metric(\"G_loss_epoch\", avg_G, step=epoch)\n",
        "\n",
        "    # save models & samples periodically\n",
        "    if epoch % SAMPLE_EVERY == 0 or epoch == 1:\n",
        "        # save checkpoints\n",
        "        torch.save(netG.state_dict(), os.path.join(MODEL_DIR, f\"netG_epoch{epoch}.pth\"))\n",
        "        torch.save(netD.state_dict(), os.path.join(MODEL_DIR, f\"netD_epoch{epoch}.pth\"))\n",
        "\n",
        "        # save as mlflow artifacts\n",
        "        mlflow.log_artifact(os.path.join(MODEL_DIR, f\"netG_epoch{epoch}.pth\"))\n",
        "        mlflow.log_artifact(os.path.join(MODEL_DIR, f\"netD_epoch{epoch}.pth\"))\n",
        "\n",
        "        # generate sample grid\n",
        "        with torch.no_grad():\n",
        "            fake_samples = netG(fixed_noise).detach().cpu()\n",
        "        grid = utils.make_grid(fake_samples, padding=2, normalize=True)\n",
        "        sample_path = os.path.join(OUT_DIR, f\"sample_epoch{epoch}.png\")\n",
        "        utils.save_image(grid, sample_path)\n",
        "        mlflow.log_artifact(sample_path)\n",
        "        print(f\"Saved sample and checkpoints for epoch {epoch}\")\n",
        "\n",
        "# end run\n",
        "mlflow.end_run()\n",
        "print(\"Training finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available: False\n",
            "No GPU detected. Running on CPU.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "\n",
        "# Get GPU name\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
        "    print(\"Total GPUs:\", torch.cuda.device_count())\n",
        "    print(\"Current Device:\", torch.cuda.current_device())\n",
        "else:\n",
        "    print(\"No GPU detected. Running on CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "mlflow.end_run()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d77d6a6a"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Load the blood cell dataset and preprocess the images for use with the vision transformer and Performer models. This will involve resizing, normalization, and splitting the data into training, validation, and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "96d53a02"
      },
      "outputs": [],
      "source": [
        "class BloodCellDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images (numpy array): Array of image data (H, W, C).\n",
        "            labels (numpy array): Array of corresponding labels.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25837724",
        "outputId": "3f5858ea-30d4-49e7-b7f4-346399aceee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training batches: 313\n",
            "Number of validation batches: 40\n",
            "Number of test batches: 40\n"
          ]
        }
      ],
      "source": [
        "# Define transformations\n",
        "# Using common mean and std dev for ImageNet as a starting point\n",
        "# A more accurate approach would be to calculate these from the dataset\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Convert numpy array to PIL Image for transforms\n",
        "    transforms.RandomResizedCrop(IMG_HEIGHT),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Convert numpy array to PIL Image for transforms\n",
        "    transforms.Resize(IMG_HEIGHT),\n",
        "    transforms.CenterCrop(IMG_HEIGHT),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "# Create Dataset instances\n",
        "train_dataset = BloodCellDataset(X_train, y_train, transform=train_transform)\n",
        "val_dataset = BloodCellDataset(X_val, y_val, transform=val_test_transform)\n",
        "test_dataset = BloodCellDataset(X_test, y_test, transform=val_test_transform)\n",
        "\n",
        "# Create DataLoader instances\n",
        "batch_size = 32 # Using the already defined batch_size\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of validation batches: {len(val_dataloader)}\")\n",
        "print(f\"Number of test batches: {len(test_dataloader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7757e7a6"
      },
      "source": [
        "## Vision transformer model implementation\n",
        "\n",
        "### Subtask:\n",
        "Implement the Vision Transformer model for blood cell classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "59d19bcc"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cae6304",
        "outputId": "9b62c8e1-8c62-4646-ce42-d544cfe90012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized and moved to cuda\n",
            "BloodCellViT(\n",
            "  (vit): VisionTransformer(\n",
            "    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "    (encoder): Encoder(\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (encoder_layer_0): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_1): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_2): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_3): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_4): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_5): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_6): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_7): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_8): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_9): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_10): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_11): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    )\n",
            "    (heads): Sequential(\n",
            "      (head): Linear(in_features=768, out_features=4, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class BloodCellViT(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(BloodCellViT, self).__init__()\n",
        "        # Load pre-trained ViT-Base/16\n",
        "        weights = ViT_B_16_Weights.DEFAULT\n",
        "        self.vit = vit_b_16(weights=None)\n",
        "        num_ftrs = self.vit.heads.head.in_features\n",
        "        self.vit.heads.head = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "\n",
        "num_classes = 4 # Based on the previous data loading step\n",
        "model = BloodCellViT(num_classes=num_classes)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model initialized and moved to {device}\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf857af2"
      },
      "source": [
        "## Performer model implementation\n",
        "\n",
        "### Subtask:\n",
        "Implement the Performer model for blood cell classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "71e4448e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "def linear_attention(q, k, v):\n",
        "    attention_weights = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attention_weights = F.softmax(attention_weights, dim=-1)\n",
        "    output = torch.matmul(attention_weights, v)\n",
        "    return output\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class PerformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4., drop_rate=0., attn_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=attn_drop_rate, batch_first=True) # Using standard MultiheadAttention for now, would replace with efficient attention\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_rate),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(drop_rate)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class PerformerModel(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_layers=12, num_heads=12, mlp_ratio=4., num_classes=4, drop_rate=0., attn_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            PerformerBlock(embed_dim, num_heads, mlp_ratio, drop_rate, attn_drop_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return x[:, 0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a321914",
        "outputId": "4381058b-dd34-4c44-e23b-500690b67d5d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'IMG_HEIGHT' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m img_size \u001b[38;5;241m=\u001b[39m \u001b[43mIMG_HEIGHT\u001b[49m\n\u001b[0;32m      2\u001b[0m patch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m      3\u001b[0m in_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'IMG_HEIGHT' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "img_size = IMG_HEIGHT\n",
        "patch_size = 16\n",
        "in_channels = 3\n",
        "embed_dim = 768\n",
        "num_layers = 12\n",
        "num_heads = 12\n",
        "mlp_ratio = 4\n",
        "num_classes = num_classes\n",
        "\n",
        "performer_model = PerformerModel(\n",
        "    img_size=img_size,\n",
        "    patch_size=patch_size,\n",
        "    in_channels=in_channels,\n",
        "    embed_dim=embed_dim,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads,\n",
        "    mlp_ratio=mlp_ratio,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "performer_model.to(device)\n",
        "\n",
        "print(f\"Performer Model initialized and moved to {device}\")\n",
        "print(performer_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e25d02f9"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train both the Vision Transformer and Performer models on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkHFKZhWvaOR"
      },
      "source": [
        "# Newly Initialized ViT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzurHPcRpaif",
        "outputId": "a7f67a71-8636-49b8-cc40-519b54217b77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|| 313/313 [06:24<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5]  Loss: 1.4277, Train Acc: 28.13%\n",
            "Validation Accuracy: 56.12%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|| 313/313 [06:24<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5]  Loss: 0.8736, Train Acc: 62.50%\n",
            "Validation Accuracy: 83.21%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|| 313/313 [06:24<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5]  Loss: 0.6239, Train Acc: 72.64%\n",
            "Validation Accuracy: 84.73%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|| 313/313 [06:25<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5]  Loss: 0.7036, Train Acc: 69.95%\n",
            "Validation Accuracy: 86.89%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|| 313/313 [06:28<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5]  Loss: 0.5512, Train Acc: 75.38%\n",
            "Validation Accuracy: 86.49%\n",
            "\n",
            " Final ViT Test Accuracy: 84.74%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_vit(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}]  Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "        val_acc = test_vit(model, val_loader, device)\n",
        "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_vit(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 4\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "vit_model = BloodCellViT(num_classes)\n",
        "optimizer = optim.AdamW(vit_model.parameters(), lr=1e-4)\n",
        "\n",
        "vit_model = train_vit(vit_model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=5)\n",
        "vit_test_acc = test_vit(vit_model, test_dataloader, device)\n",
        "print(f\"\\n Final ViT Test Accuracy: {vit_test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PGfmoVuWvJuK"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'performer_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Assume 'performer_model' is your trained model object\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mperformer_model\u001b[49m\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#  1. Save model to disk\u001b[39;00m\n\u001b[0;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(performer_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/performer_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'performer_model' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "# Assume 'performer_model' is your trained model object\n",
        "performer_model.eval()  # Set to evaluation mode\n",
        "\n",
        "#  1. Save model to disk\n",
        "torch.save(performer_model.state_dict(), \"models/performer_model.pth\")\n",
        "print(\" Performer model saved to disk as performer_model.pth\")\n",
        "\n",
        "#  2. Log model to MLflow\n",
        "mlflow.set_tracking_uri(\"file:///D:/Blood Cell Classifiaction/blood-cell-classification/mlruns\")\n",
        "mlflow.set_experiment(\"Blood-Cell-Classification-Results\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"Performer-Model-Logged\"):\n",
        "    mlflow.pytorch.log_model(performer_model, artifact_path=\"performer_model\")\n",
        "    \n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"model_type\", \"Performer (Efficient Transformer)\")\n",
        "    mlflow.log_param(\"epochs\", 5)\n",
        "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "    mlflow.log_param(\"num_layers\", 12)\n",
        "    mlflow.log_param(\"embed_dim\", 768)\n",
        "    mlflow.log_param(\"num_heads\", 12)\n",
        "    \n",
        "    #  3. Log metrics per epoch\n",
        "    epoch_metrics = [\n",
        "        {\"loss\":1.4277, \"train_acc\":28.13, \"val_acc\":56.12},\n",
        "        {\"loss\":0.8736, \"train_acc\":62.50, \"val_acc\":83.21},\n",
        "        {\"loss\":0.6239, \"train_acc\":72.64, \"val_acc\":84.73},\n",
        "        {\"loss\":0.7036, \"train_acc\":69.95, \"val_acc\":86.89},\n",
        "        {\"loss\":0.5512, \"train_acc\":75.38, \"val_acc\":86.49}\n",
        "    ]\n",
        "    \n",
        "    for i, metrics in enumerate(epoch_metrics, start=1):\n",
        "        mlflow.log_metric(\"loss\", metrics[\"loss\"], step=i)\n",
        "        mlflow.log_metric(\"train_acc\", metrics[\"train_acc\"], step=i)\n",
        "        mlflow.log_metric(\"val_acc\", metrics[\"val_acc\"], step=i)\n",
        "    \n",
        "    # Final test accuracy\n",
        "    mlflow.log_metric(\"final_test_accuracy\", 84.74)\n",
        "\n",
        "print(\" Performer model logged to MLflow successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LvukEj9vK9c"
      },
      "source": [
        "# **PreTrained ViT Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iETPbvLPvFUe",
        "outputId": "c7e47edb-97a5-4716-9ae4-428b750766aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|| 313/313 [06:29<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5]  Loss: 0.5570, Train Acc: 75.55%\n",
            "Validation Accuracy: 91.93%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|| 313/313 [06:27<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5]  Loss: 0.3417, Train Acc: 85.24%\n",
            "Validation Accuracy: 96.56%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|| 313/313 [06:28<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5]  Loss: 0.3072, Train Acc: 87.06%\n",
            "Validation Accuracy: 96.24%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|| 313/313 [06:27<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5]  Loss: 0.2820, Train Acc: 87.87%\n",
            "Validation Accuracy: 96.64%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|| 313/313 [06:28<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5]  Loss: 0.2511, Train Acc: 89.58%\n",
            "Validation Accuracy: 97.04%\n",
            "\n",
            " Final ViT Test Accuracy: 97.12%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "class BloodCellViT(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(BloodCellViT, self).__init__()\n",
        "        # Pretrained ViT model\n",
        "        weights = ViT_B_16_Weights.DEFAULT\n",
        "        self.vit = vit_b_16(weights=weights)\n",
        "        num_ftrs = self.vit.heads.head.in_features\n",
        "        self.vit.heads.head = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "def test_vit(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "def train_vit(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.to(device)\n",
        "    \n",
        "    # START MLFLOW RUN\n",
        "    with mlflow.start_run(run_name=f\"ViT-PreTrained-{epochs}epochs\"):\n",
        "        \n",
        "        # Log hyperparameters\n",
        "        mlflow.log_param(\"model_type\", \"Vision Transformer (PreTrained)\")\n",
        "        mlflow.log_param(\"epochs\", epochs)\n",
        "        mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "        mlflow.log_param(\"batch_size\", train_loader.batch_size)\n",
        "        mlflow.log_param(\"device\", str(device))\n",
        "        mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "        \n",
        "        print(f\"\\n Starting ViT Training with MLflow tracking...\")\n",
        "        print(f\"   Run name: ViT-PreTrained-{epochs}epochs\")\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "            for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [TRAIN]\"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "            train_acc = 100 * correct / total\n",
        "            train_loss = running_loss / len(train_loader)\n",
        "            \n",
        "            # Validation\n",
        "            val_acc = test_vit(model, val_loader, device)\n",
        "            \n",
        "            print(f\"Epoch [{epoch+1}/{epochs}]  Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "            \n",
        "            # Log metrics to MLflow\n",
        "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
        "            mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
        "        \n",
        "        # SAVE AND LOG THE MODEL\n",
        "        print(f\"\\n Saving model to MLflow...\")\n",
        "        mlflow.pytorch.log_model(model, artifact_path=\"vit_model\")\n",
        "        \n",
        "        print(f\" MLflow run completed! View at http://localhost:5000\")\n",
        "        \n",
        "    return model\n",
        "\n",
        "# Initialize\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 4\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"\\n Model Setup:\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Classes: {num_classes}\")\n",
        "\n",
        "vit_model = BloodCellViT(num_classes)\n",
        "optimizer = optim.AdamW(vit_model.parameters(), lr=1e-4)\n",
        "\n",
        "# Train with MLflow tracking\n",
        "vit_model = train_vit(vit_model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=5)\n",
        "\n",
        "# Final test\n",
        "vit_test_acc = test_vit(vit_model, test_dataloader, device)\n",
        "print(f\"\\n Final ViT Test Accuracy: {vit_test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ7d8lmATmL5",
        "outputId": "08acdace-1299-4f86-bc65-b9bb5bcd3446"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5]  Loss: 1.4087, Train Acc: 30.07%\n",
            "Validation Accuracy: 46.52%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5]  Loss: 0.9530, Train Acc: 57.67%\n",
            "Validation Accuracy: 77.30%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5]  Loss: 0.6807, Train Acc: 70.94%\n",
            "Validation Accuracy: 83.37%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|| 313/313 [06:19<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5]  Loss: 0.5925, Train Acc: 74.37%\n",
            "Validation Accuracy: 87.77%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|| 313/313 [06:21<00:00,  1.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5]  Loss: 0.5276, Train Acc: 76.98%\n",
            "Validation Accuracy: 88.09%\n",
            "\n",
            " Final Performer Test Accuracy: 88.90%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "def test_performer(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "def train_performer(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.to(device)\n",
        "    \n",
        "    # START MLFLOW RUN\n",
        "    with mlflow.start_run(run_name=f\"Performer-{epochs}epochs\"):\n",
        "        \n",
        "        # Log hyperparameters\n",
        "        mlflow.log_param(\"model_type\", \"Performer\")\n",
        "        mlflow.log_param(\"epochs\", epochs)\n",
        "        mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "        mlflow.log_param(\"batch_size\", train_loader.batch_size)\n",
        "        mlflow.log_param(\"device\", str(device))\n",
        "        mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "        mlflow.log_param(\"num_layers\", 12)\n",
        "        mlflow.log_param(\"embed_dim\", 768)\n",
        "        \n",
        "        print(f\"\\n Starting Performer Training with MLflow tracking...\")\n",
        "        print(f\"   Run name: Performer-{epochs}epochs\")\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "            for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [TRAIN]\"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "            train_acc = 100 * correct / total\n",
        "            train_loss = running_loss / len(train_loader)\n",
        "            \n",
        "            # Validation\n",
        "            val_acc = test_performer(model, val_loader, device)\n",
        "            \n",
        "            print(f\"Epoch [{epoch+1}/{epochs}]  Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "            \n",
        "            # Log metrics to MLflow\n",
        "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
        "            mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
        "        \n",
        "        # SAVE AND LOG THE MODEL\n",
        "        print(f\"\\n Saving model to MLflow...\")\n",
        "        mlflow.pytorch.log_model(model, artifact_path=\"performer_model\")\n",
        "        \n",
        "        print(f\" MLflow run completed! View at http://localhost:5000\")\n",
        "        \n",
        "    return model\n",
        "\n",
        "# Initialize\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(performer_model.parameters(), lr=1e-4)\n",
        "\n",
        "print(f\"\\n Performer Model Setup:\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "# Train with MLflow tracking\n",
        "performer_model = train_performer(performer_model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=5)\n",
        "\n",
        "# Final test\n",
        "performer_test_acc = test_performer(performer_model, test_dataloader, device)\n",
        "print(f\"\\n Final Performer Test Accuracy: {performer_test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_hI7PM9TmgQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU1JdNztv9WB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Va3BFeGQTmjn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLflow Tracking URI: file:///d:/Blood%20Cell%20Classifiaction/blood-cell-classification/notebooks/mlruns\n",
            " Successfully connected to MLflow!\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "\n",
        "# Test logging to verify connection\n",
        "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "with mlflow.start_run():\n",
        "    print(\" Successfully connected to MLflow!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/11/30 00:55:26 INFO mlflow.tracking.fluent: Experiment with name 'MLflow Quickstart' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='file:///d:/Blood%20Cell%20Classifiaction/blood-cell-classification/notebooks/mlruns/386924970153409238', creation_time=1764444326364, experiment_id='386924970153409238', last_update_time=1764444326364, lifecycle_stage='active', name='MLflow Quickstart', tags={}>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import mlflow\n",
        "\n",
        "mlflow.set_experiment(\"MLflow Quickstart\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting protobuf==3.20.3"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.38.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "databricks-sdk 0.73.0 requires protobuf!=5.26.*,!=5.27.*,!=5.28.*,!=5.29.0,!=5.29.1,!=5.29.2,!=5.29.3,!=5.29.4,!=6.30.0,!=6.30.1,!=6.31.0,<7.0,>=4.25.8, but you have protobuf 3.20.3 which is incompatible.\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sunil kumar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
            "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\Sunil Kumar\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 6.33.1\n",
            "    Uninstalling protobuf-6.33.1:\n",
            "      Successfully uninstalled protobuf-6.33.1\n",
            "Successfully installed protobuf-3.20.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade protobuf==3.20.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracking URI: file:///d:/Blood%20Cell%20Classifiaction/blood-cell-classification/notebooks/mlruns\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "print(\"Tracking URI:\", mlflow.get_tracking_uri())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            " LOGGING TRAINED MODEL RESULTS TO MLFLOW\n",
            "================================================================================\n",
            "\n",
            " Logging Custom ViT Model Results...\n",
            " Custom ViT results logged!\n",
            "\n",
            " Logging PreTrained ViT Model Results...\n",
            " PreTrained ViT results logged!\n",
            "\n",
            " Logging Performer Model Results...\n",
            " Performer results logged!\n",
            "\n",
            "================================================================================\n",
            " ALL RESULTS LOGGED TO MLFLOW!\n",
            "================================================================================\n",
            "\n",
            " SUMMARY:\n",
            "   Custom ViT:        Final Test Accuracy = 84.74%\n",
            "   PreTrained ViT:    Final Test Accuracy = 97.12%  BEST\n",
            "   Performer Model:   Final Test Accuracy = 88.90%\n",
            "\n",
            " Open MLflow Dashboard:\n",
            "    http://localhost:5000\n",
            "\n",
            " Experiment: Blood-Cell-Classification-Results\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "# Set MLflow tracking URI\n",
        "mlflow.set_tracking_uri(\"file:///D:/Blood Cell Classifiaction/blood-cell-classification/mlruns\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" LOGGING TRAINED MODEL RESULTS TO MLFLOW\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "mlflow.set_experiment(\"Blood-Cell-Classification-Results\")\n",
        "\n",
        "# \n",
        "# 1 CUSTOM VIT MODEL RESULTS\n",
        "# \n",
        "print(\"\\n Logging Custom ViT Model Results...\")\n",
        "with mlflow.start_run(run_name=\"Custom-ViT-Model\"):\n",
        "    mlflow.log_param(\"model_type\", \"Custom Vision Transformer\")\n",
        "    mlflow.log_param(\"epochs\", 5)\n",
        "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "\n",
        "    # Epoch-wise metrics\n",
        "    loss_list = [1.4277, 0.8736, 0.6239, 0.7036, 0.5512]\n",
        "    train_acc_list = [28.13, 62.50, 72.64, 69.95, 75.38]\n",
        "    val_acc_list = [56.12, 83.21, 84.73, 86.89, 86.49]\n",
        "\n",
        "    for epoch in range(5):\n",
        "        mlflow.log_metric(\"loss\", loss_list[epoch], step=epoch+1)\n",
        "        mlflow.log_metric(\"train_accuracy\", train_acc_list[epoch], step=epoch+1)\n",
        "        mlflow.log_metric(\"val_accuracy\", val_acc_list[epoch], step=epoch+1)\n",
        "\n",
        "    mlflow.log_metric(\"final_test_accuracy\", 84.74)\n",
        "    print(\" Custom ViT results logged!\")\n",
        "\n",
        "# \n",
        "# 2 PRETRAINED VIT MODEL RESULTS\n",
        "# \n",
        "print(\"\\n Logging PreTrained ViT Model Results...\")\n",
        "with mlflow.start_run(run_name=\"PreTrained-ViT-Model\"):\n",
        "    mlflow.log_param(\"model_type\", \"Vision Transformer (PreTrained ViT-B/16)\")\n",
        "    mlflow.log_param(\"epochs\", 5)\n",
        "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "    mlflow.log_param(\"pretrained_weights\", \"ViT_B_16_Weights.DEFAULT\")\n",
        "\n",
        "    # Epoch-wise metrics\n",
        "    loss_list = [0.5570, 0.3417, 0.3072, 0.2820, 0.2511]\n",
        "    train_acc_list = [75.55, 85.24, 87.06, 87.87, 89.58]\n",
        "    val_acc_list = [91.93, 96.56, 96.24, 96.64, 97.04]\n",
        "\n",
        "    for epoch in range(5):\n",
        "        mlflow.log_metric(\"loss\", loss_list[epoch], step=epoch+1)\n",
        "        mlflow.log_metric(\"train_accuracy\", train_acc_list[epoch], step=epoch+1)\n",
        "        mlflow.log_metric(\"val_accuracy\", val_acc_list[epoch], step=epoch+1)\n",
        "\n",
        "    mlflow.log_metric(\"final_test_accuracy\", 97.12)\n",
        "    print(\" PreTrained ViT results logged!\")\n",
        "\n",
        "# \n",
        "# 3 PERFORMER MODEL RESULTS\n",
        "# \n",
        "print(\"\\n Logging Performer Model Results...\")\n",
        "with mlflow.start_run(run_name=\"Performer-Model\"):\n",
        "    mlflow.log_param(\"model_type\", \"Performer (Efficient Transformer)\")\n",
        "    mlflow.log_param(\"epochs\", 5)\n",
        "    mlflow.log_param(\"optimizer\", \"AdamW\")\n",
        "    mlflow.log_param(\"learning_rate\", 1e-4)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "    mlflow.log_param(\"num_layers\", 12)\n",
        "    mlflow.log_param(\"embed_dim\", 768)\n",
        "    mlflow.log_param(\"num_heads\", 12)\n",
        "\n",
        "    # Epoch-wise metrics\n",
        "    loss_list = [1.4087, 0.9530, 0.6807, 0.5925, 0.5276]\n",
        "    train_acc_list = [30.07, 57.67, 70.94, 74.37, 76.98]\n",
        "    val_acc_list = [46.52, 77.30, 83.37, 87.77, 88.09]\n",
        "\n",
        "    for epoch in range(5):\n",
        "        mlflow.log_metric(\"loss\", loss_list[epoch], step=epoch+1)\n",
        "        mlflow.log_metric(\"train_accuracy\", train_acc_list[epoch], step=epoch+1)\n",
        "        mlflow.log_metric(\"val_accuracy\", val_acc_list[epoch], step=epoch+1)\n",
        "\n",
        "    mlflow.log_metric(\"final_test_accuracy\", 88.90)\n",
        "    print(\" Performer results logged!\")\n",
        "\n",
        "# \n",
        "# SUMMARY\n",
        "# \n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" ALL RESULTS LOGGED TO MLFLOW!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n SUMMARY:\")\n",
        "print(f\"   Custom ViT:        Final Test Accuracy = 84.74%\")\n",
        "print(f\"   PreTrained ViT:    Final Test Accuracy = 97.12%  BEST\")\n",
        "print(f\"   Performer Model:   Final Test Accuracy = 88.90%\")\n",
        "print(\"\\n Open MLflow Dashboard:\")\n",
        "print(\"    http://localhost:5000\")\n",
        "print(\"\\n Experiment: Blood-Cell-Classification-Results\")\n",
        "print(\"=\"*80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracking URI: file:///D:/Blood Cell Classifiaction/blood-cell-classification/mlruns\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "print(\"Tracking URI:\", mlflow.get_tracking_uri())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "mlflow.set_tracking_uri(\"file:///D:/Blood Cell Classifiaction/blood-cell-classification/mlruns\")\n",
        "mlflow.set_experiment(\"Blood-Cell-Classification-Results\")\n",
        "\n",
        "def log_model_results(run_name, model_type, epochs, metrics, params=None):\n",
        "    \"\"\"Log metrics and parameters for a model to MLflow.\"\"\"\n",
        "    if params is None:\n",
        "        params = {}\n",
        "    \n",
        "    with mlflow.start_run(run_name=run_name):\n",
        "        # Log parameters\n",
        "        mlflow.log_param(\"model_type\", model_type)\n",
        "        mlflow.log_param(\"epochs\", epochs)\n",
        "        for key, val in params.items():\n",
        "            mlflow.log_param(key, val)\n",
        "        \n",
        "        # Log metrics per epoch\n",
        "        for metric_type in [\"train_acc\", \"val_acc\", \"loss\"]:\n",
        "            if metric_type in metrics:\n",
        "                for epoch in range(1, epochs + 1):\n",
        "                    key = f\"epoch_{epoch}_{metric_type}\"\n",
        "                    if key in metrics[metric_type]:\n",
        "                        mlflow.log_metric(metric_type, metrics[metric_type][key], step=epoch)\n",
        "        \n",
        "        # Log final test accuracy\n",
        "        if \"final_test_accuracy\" in metrics:\n",
        "            mlflow.log_metric(\"final_test_accuracy\", metrics[\"final_test_accuracy\"])\n",
        "\n",
        "# Example: Custom ViT\n",
        "custom_vit_metrics = {\n",
        "    \"train_acc\": {\n",
        "        \"epoch_1_train_acc\":28.13, \"epoch_2_train_acc\":62.50,\n",
        "        \"epoch_3_train_acc\":72.64, \"epoch_4_train_acc\":69.95,\n",
        "        \"epoch_5_train_acc\":75.38\n",
        "    },\n",
        "    \"val_acc\": {\n",
        "        \"epoch_1_val_acc\":56.12, \"epoch_2_val_acc\":83.21,\n",
        "        \"epoch_3_val_acc\":84.73, \"epoch_4_val_acc\":86.89,\n",
        "        \"epoch_5_val_acc\":86.49\n",
        "    },\n",
        "    \"loss\": {\n",
        "        \"epoch_1_loss\":1.4277, \"epoch_2_loss\":0.8736,\n",
        "        \"epoch_3_loss\":0.6239, \"epoch_4_loss\":0.7036,\n",
        "        \"epoch_5_loss\":0.5512\n",
        "    },\n",
        "    \"final_test_accuracy\": 84.74\n",
        "}\n",
        "custom_params = {\"optimizer\":\"AdamW\",\"learning_rate\":1e-4,\"batch_size\":32}\n",
        "\n",
        "log_model_results(\"Custom-ViT-Model\", \"Custom Vision Transformer\", 5, custom_vit_metrics, custom_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
